# Spark (Bitnami) values template for the project

global:
  defaultStorageClass: $DEFAULT_STORAGE_CLASS

kubeVersion: ""

nameOverride: "spark"
fullnameOverride: "spark"

# Run Spark on node3 alongside Airflow
master:
  enabled: true
  # Pin master to node3
  nodeSelector:
    kubernetes.io/hostname: $NODE3_NAME
  # Give master modest resources
  resourcesPreset: "small"
  resources: {}
  # Expose Spark master ports internally
  containerPorts:
    http: 8080
    https: 8480
    cluster: 7077

  # Hadoop s3a + MinIO config
  configOptions: >-
    -Dspark.hadoop.fs.s3a.endpoint=$MINIO_ENDPOINT
    -Dspark.hadoop.fs.s3a.path.style.access=true
    -Dspark.hadoop.fs.s3a.connection.ssl.enabled=$MINIO_USE_SSL
    -Dspark.hadoop.fs.s3a.access.key=${MINIO_ACCESS_KEY}
    -Dspark.hadoop.fs.s3a.secret.key=${MINIO_SECRET_KEY}
    -Dspark.driver.extraClassPath=/opt/bitnami/spark/extra-jars/*
    -Dspark.executor.extraClassPath=/opt/bitnami/spark/extra-jars/*

  # Export credentials also as env vars (AWS SDK fallback)
  extraEnvVars:
    - name: AWS_ACCESS_KEY_ID
      value: $MINIO_ACCESS_KEY
    - name: AWS_SECRET_ACCESS_KEY
      value: $MINIO_SECRET_KEY
    - name: AWS_EC2_METADATA_DISABLED
      value: "true"
    - name: SPARK_EXTRA_CLASSPATH
      value: "/opt/bitnami/spark/extra-jars/*"

  # Download required JARs (MSSQL JDBC + Hadoop AWS) into a shared emptyDir
  initContainers:
    - name: init-extra-jars
      image: docker.io/curlimages/curl:8.8.0
      imagePullPolicy: IfNotPresent
      command:
        - /bin/sh
        - -ec
        - |
          # --- Download MSSQL JDBC driver (compatible with Java 17) ---
          # NOTE: update version if you need newer driver
          curl -fL -o /jars/mssql-jdbc-12.6.1.jre11.jar \
            https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/12.6.1.jre11/mssql-jdbc-12.6.1.jre11.jar

          # --- Download Hadoop AWS + AWS SDK bundle ---
          # Use versions compatible with Spark's bundled Hadoop (3.3.x for Bitnami Spark 4.x)
          curl -fL -o /jars/hadoop-aws-3.3.4.jar \
            https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
          curl -fL -o /jars/aws-java-sdk-bundle-1.12.262.jar \
            https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar

          # --- Basic check ---
          ls -lh /jars
      volumeMounts:
        - name: extra-jars
          mountPath: /jars

  extraVolumes:
    - name: extra-jars
      emptyDir: {}

  extraVolumeMounts:
    - name: extra-jars
      mountPath: /opt/bitnami/spark/extra-jars

  networkPolicy:
    enabled: true
    allowExternal: true
    allowExternalEgress: true

  pdb:
    create: true

worker:
  enabled: true
  # Pin workers to node3 as well
  nodeSelector:
    kubernetes.io/hostname: $NODE3_NAME
  replicaCount: 2

  # Modest defaults; raise for bigger jobs
  resourcesPreset: "small"
  resources: {}
  startupProbe:
    enabled: true

  # Same Hadoop s3a + classpath config on workers
  configOptions: >-
    -Dspark.hadoop.fs.s3a.endpoint=$MINIO_ENDPOINT
    -Dspark.hadoop.fs.s3a.path.style.access=true
    -Dspark.hadoop.fs.s3a.connection.ssl.enabled=$MINIO_USE_SSL
    -Dspark.hadoop.fs.s3a.access.key=$MINIO_ACCESS_KEY
    -Dspark.hadoop.fs.s3a.secret.key=$MINIO_SECRET_KEY
    -Dspark.driver.extraClassPath=/opt/bitnami/spark/extra-jars/*
    -Dspark.executor.extraClassPath=/opt/bitnami/spark/extra-jars/*

  extraEnvVars:
    - name: AWS_ACCESS_KEY_ID
      value: $MINIO_ACCESS_KEY
    - name: AWS_SECRET_ACCESS_KEY
      value: $MINIO_SECRET_KEY
    - name: AWS_EC2_METADATA_DISABLED
      value: "true"
    - name: SPARK_EXTRA_CLASSPATH
      value: "/opt/bitnami/spark/extra-jars/*"

  initContainers:
    - name: init-extra-jars
      image: docker.io/curlimages/curl:8.8.0
      imagePullPolicy: IfNotPresent
      command:
        - /bin/sh
        - -ec
        - |
          curl -fL -o /jars/mssql-jdbc-12.6.1.jre11.jar \
            https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/12.6.1.jre11/mssql-jdbc-12.6.1.jre11.jar
          curl -fL -o /jars/hadoop-aws-3.3.4.jar \
            https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
          curl -fL -o /jars/aws-java-sdk-bundle-1.12.262.jar \
            https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar
          ls -lh /jars
      volumeMounts:
        - name: extra-jars
          mountPath: /jars

  extraVolumes:
    - name: extra-jars
      emptyDir: {}

  extraVolumeMounts:
    - name: extra-jars
      mountPath: /opt/bitnami/spark/extra-jars

  networkPolicy:
    enabled: true
    allowExternal: true
    allowExternalEgress: true

  pdb:
    create: true

# Service for Spark master (NodePort for external UI access)
service:
  type: NodePort
  ports:
    http: 80
    https: 443
    cluster: 7077
  nodePorts:
    http: $SPARK_NODEPORT_HTTP
    https: ""
    cluster: ""
  annotations: {}

# Image: stick to Bitnami Spark 4.x
image:
  registry: docker.io
  repository: bitnami/spark
  tag: 4.0.0-debian-12-r20
  pullPolicy: IfNotPresent
  debug: true

serviceAccount:
  create: true
  automountServiceAccountToken: false

metrics:
  enabled: false